{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Colab, comment out if not using Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line if you're using Colab to something like '/content/drive/MyDrive/TeamX/'\n",
    "# where TeamX is just the clone of repository on your Google Drive\n",
    "# and you have mounted the drive at /content/drive  \n",
    "# See the Tutorial Slides for more detail.\n",
    "\n",
    "# Works on your local machine but not on Colab!\n",
    "PROJECT_ROOT = '../..' \n",
    "\n",
    "# Fix this path and use this one on Colab\n",
    "# PROJECT_ROOT = '/content/drive/MyDrive/TeamX' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os.path import join as ospj\n",
    "\n",
    "sys.path.append(ospj(PROJECT_ROOT, 'src'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nO23A_d-c4t8"
   },
   "source": [
    "In Exercise 3, you will implement a convolutional neural network to perform image classification and explore methods to improve the training performance and generalization of these networks.\n",
    "We will use the CIFAR-10 dataset as a benchmark for our networks, similar to the previous exercise. This dataset consists of 50000 training images of 32x32 resolution with 10 object classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The task is to implement a convolutional network to classify these images using the PyTorch library. The four questions are,\n",
    "\n",
    "- Implementing a convolutional neural network, training it, and visualizing its weights (Question 1).\n",
    "- Experiment with batch normalization and early stopping (Question 2).\n",
    "- Data augmentation and dropout to improve generalization (Question 3).\n",
    "- Implement transfer learning from an ImageNet-pretrained model (Question 4)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cpyHKxVkc4t-"
   },
   "source": [
    "### Question 1: Implement Convolutional Network (10 points)\n",
    "\n",
    "In this question, we will implement a five-layered convolutional neural network architecture as well as the loss function to train it. Refer to the comments in the code to the exact places where you need to fill in the code.\n",
    "\n",
    "![](../../data/exercise-3/fig1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsjBOeJc4uA"
   },
   "source": [
    "a) Our architecture is shown in Fig 1. It has five convolution blocks. Each block is consist of convolution, max pooling, and ReLU operation in that order. We will use 3×3 kernels in all convolutional layers. Set the padding and stride of the convolutional layers so that they maintain the spatial dimensions. Max pooling operations are done with 2×2 kernels, with a stride of 2, thereby halving the spatial resolution each time. Finally, stacking these five blocks leads to a 512 × 1 × 1 feature map. Classification is achieved by a fully connected layer. We will train convolutional neural networks on the CIFAR-10 dataset. Implement a class ConvNet to define the model described. The ConvNet takes 32 × 32 color images as inputs and has 5 hidden layers with 128, 512, 512, 512, 512 filters, and produces a 10-class classification. The code to train the model is already provided. Train the above model and report the training and validation accuracies. (5 points)\n",
    "\n",
    "Please implement the above network (initialization and forward pass) in class `ConvNet` in `models/cnn/model.py`.\n",
    "\n",
    "b) Implement the method `__str__` in `models/base_model.py`, which should give a string representaiton of the model. The string should show the number of `trainable` parameters for each layer. This gives us a measure of model capacity. Also at the end, the string contains the total number of trainable parameters for the entire model. (2 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hDrZOaNYc4uC"
   },
   "source": [
    "![](../../data/exercise-3/fig2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gyQqsQZVc4uD",
    "tags": [
     "batch norm"
    ]
   },
   "source": [
    "c) Implement a function `VisualizeFilter` in `models/cnn/model.py`, which visualizes the filters of the first convolution layer implemented in Q1.a. In other words, you need to show 128 filters with size 3x3 as color images (since each filter has three input channels). Stack these into 3x3 color images into one large image. You can use the `imshow` function from the `matplotlib` library to visualize the weights. See an example in Fig. 2 Compare the filters before and after training. Do you see any patterns? (3 points). Please attach your output images before and after training in a cell with your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "from utils.parse_config import ConfigParser\n",
    "from trainers.cnn_trainer import CNNTrainer\n",
    "import data_loaders.data_modules as module_data\n",
    "\n",
    "from copy import deepcopy\n",
    "%aimport -ConfigParser # Due to an issue of pickle and auto_reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformations for split train are Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n",
      "Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n",
      "transformations for split eval are Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Initialization DataLoader for 10000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser.wo_args(config='cfgs/exercise-3/cnn_cifar10.json', root_dir=PROJECT_ROOT)\n",
    "\n",
    "datamodule = config.init_obj('data_module', module_data,\n",
    "                             root_dir=PROJECT_ROOT #Just add the root_dir to the rest of the arguments defined in config file\n",
    "                            )\n",
    "\n",
    "# Based on the heldout_split in the config file, \n",
    "# the datamodule will break the dataset into two splits\n",
    "train_data_loader = datamodule.get_loader()\n",
    "valid_data_loader = datamodule.get_heldout_loader()\n",
    "\n",
    "# Test loader is the same as train loader\n",
    "# except that training=False, shuffle=False, and no splitting is done \n",
    "# So we use the exact config from training and just modify these arguments\n",
    "test_loader_args = deepcopy(config['data_module']['args']) #copy the args\n",
    "test_loader_args['training']=False\n",
    "test_loader_args['shuffle']=False\n",
    "test_loader_args['heldout_split']=0.0\n",
    "\n",
    "# Now we initialize the test module with the modified config\n",
    "test_module = getattr(module_data, config['data_module']['type'])(root_dir=PROJECT_ROOT, **test_loader_args)\n",
    "# And get the loader from it\n",
    "test_loader = test_module.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There's no GPU available on this machine,training will be performed on CPU.\n",
      "ConvNet(\n",
      "  (_activation): ReLU()\n",
      "  (_norm_layer): Identity()\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Identity()\n",
      "    (5): Dropout2d(p=0.1, inplace=False)\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Identity()\n",
      "    (9): Dropout2d(p=0.1, inplace=False)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Identity()\n",
      "    (13): Dropout2d(p=0.1, inplace=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU()\n",
      "    (16): Identity()\n",
      "    (17): Dropout2d(p=0.1, inplace=False)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU()\n",
      "    (20): Identity()\n",
      "    (21): Dropout2d(p=0.1, inplace=False)\n",
      "    (22): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_cnn = CNNTrainer(config=config, train_loader=train_data_loader, eval_loader=valid_data_loader)\n",
    "\n",
    "trainer_cnn.model.VisualizeFilter()\n",
    "trainer_cnn.train()\n",
    "trainer_cnn.model.VisualizeFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: ../../saved/models/CIFAR10_CNN/0523_213724/best_val_model.pth ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../saved/models/CIFAR10_CNN/0523_213724/best_val_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0523_213724\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m      3\u001b[0m path \u001b[39m=\u001b[39m ospj(PROJECT_ROOT, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msaved/models/CIFAR10_CNN/\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_dir\u001b[39m}\u001b[39;00m\u001b[39m/best_val_model.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m trainer_cnn\u001b[39m.\u001b[39;49mload_model(path\u001b[39m=\u001b[39;49mpath)\n\u001b[1;32m      7\u001b[0m result \u001b[39m=\u001b[39m trainer_cnn\u001b[39m.\u001b[39mevaluate(loader\u001b[39m=\u001b[39mtest_loader)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/team39/src/notebooks/../../src/trainers/base_trainer.py:174\u001b[0m, in \u001b[0;36mBaseTrainer.load_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mLoads model params from the given path.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m: param path: path to save model (including filename.)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mLoading checkpoint: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[0;32m--> 174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCheckpoint loaded.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hlcv-ss23/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/hlcv-ss23/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/hlcv-ss23/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../saved/models/CIFAR10_CNN/0523_213724/best_val_model.pth'"
     ]
    }
   ],
   "source": [
    "# Change this to the experiment you want to visualize (format is `MMDD_TIME`)\n",
    "checkpoint_dir = '0523_213724' \n",
    "path = ospj(PROJECT_ROOT, f'saved/models/CIFAR10_CNN/{checkpoint_dir}/best_val_model.pth')\n",
    "\n",
    "trainer_cnn.load_model(path=path)\n",
    "\n",
    "result = trainer_cnn.evaluate(loader=test_loader)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MKti0Y9ic4uK"
   },
   "source": [
    "#### Wirte your report for Q1 in this cell.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A5_tJXTmc4uK"
   },
   "source": [
    "### Question 2: Improve training of Convolutional Networks (15 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Batch normalization is a widely used operation in neural networks, which will increase the speed of convergence and reach higher performance. You can read the paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” for more theoretical details.\n",
    "In practice, these operations are implemented in most toolboxes, such as PyTorch and TensorFlow. Add batch normalization in the model of Q1.a (You can use PyTorch's implementation). Please keep other hyperparameters the same, but only add batch normalization. The ConvNet with batch normalization still uses the same class with Q1.a but different arguments. Check the code for details. In each block, the computations are in the order of convolution, batch normalization, pooling, and ReLU. Compare the loss curves and accuracy using batch normalization to its counterpart in Q1.a. (5 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wirte your report for Q2.a in this cell. Feel free to add extra code cells\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Throughout training, we optimize our parameters on the training set. This does not guarantee that with ever step we also improve on validation and test set as well! Hence, there is no reason for our latest training checkpoint (the last checkpoint after the last epoch) to be the best to keep. One simple idea is to save a checkpoint of the best model for the validation set throughout the training. Meanining that as the training proceeds, we keep checking our accuracy after each epoch (or every N epochs) and save the best model. This can mitigate overfitting, as if the model overfits to training data (and accuracy on validation set drops), we would still have access to the best model checkpoint! Note that you should not do this on the test set, as we are not alowed to optimize **anything** (including the checkpoint selection) on the test set.\n",
    "\n",
    "For this task, you need add the logic for saving the `best model` during the training. In the `src/trainers/base_trainer`, in method `train()` we have already have the call to `self.evaluate()`. All you need to add is to process the returned result (a dictionary of metric_key -> metric_value) and see if you should save a checkpoint of the model. If yes, then you can save a checkpoint at `self.checkpoint_dir` under `best_val_model.pth` or a similar name, using the `save_model()` method. Feel free to define additional class attributes or methods if needed.\n",
    "We also recommend adding a few prints, such as the epochs that you save the best model at.\n",
    "\n",
    "Increase the training epochs to 50 in Q1.a and Q2.a, and compare the **best model** and **latest model** on the **training set** and **validation set**. Due to the randomness, you can train multiple times to verify and observe overfitting and early stopping. (5 points)\n",
    "Feel free to add any needed train/evaluation code below for this task.\n",
    "\n",
    "You can also add extra code to `base_trainer.py` so that it returns extra information after the training is finished. For example, in assignment 2's `models/twolayernet/model.py` we had a train method that would return the history of loss values, and then in the notebook the history was plotted. Feel free to make adjustments that let you better understand what's happening. This also applies to next questions. Right now the code only uses tensorboard and wandb for plotting (if enabled in config). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wWeRAObUc4uL"
   },
   "source": [
    "Wirte your report for Q2.b in this cell. Feel free to add extra code cells\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) While in part `b` we save the best model, we still do as many epochs as indicated in the config file. This is not convenient as the overfitting steps are wasting time and compute and also wouldn't affect the best model. Hence, Early Stopping can be helpful, where we **stop** the training after a few non-improving steps! Early stopping logic should be considered after every training epoch is finished, to see if we should do more epochs or not. Therefore, the logic should should be implemented in `train` method in `base_trainer.py` (which takes care of running multiple epochs). You need to implement it at the end of the loop over epochs.\n",
    "\n",
    "Once implemented, you need to change the config file to enable early stopping. You can either modify `cfgs/exercise-3/cnn_cifar10.json` directly or simply create a copy at the same place under a different name and parse that config instead. Within the config file, modify the following \"trainer\" configs:\n",
    "```JSON\n",
    "{\n",
    "    ...\n",
    "    \"trainer\":{\n",
    "        ...\n",
    "        \"monitor\": \"max eval_top1\",\n",
    "        \"early_stop\": 0 -> change to 4\n",
    "    }\n",
    "}\n",
    "```\n",
    "This will enable the early stopping to be considered for `eval_top1` metric and the maximum number of non-improving steps will be set to 4.\n",
    "\n",
    "Re-run one of the experiments from part `b` that the best epoch was way lower than the total number of epochs, and see if early stopping can prevent unnecessary training epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wirte your report for Q2.c in this cell. Feel free to add extra code cells\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qc1RwzMnc4uL"
   },
   "source": [
    "### Question 3: Improve generalization of Convolutional Networks (10 points)\n",
    "\n",
    "We saw in Q2 that the model can start over-fitting to the training set if we continue training for long. To prevent over-fitting, there are two main paradigms we can focus on. The first is to get more training data. This might be a difficult and expensive process involving significant. However, it is generally the most effective way to learn more general models. A cheaper alternative is to perform data augmentation. The second approach is to regularize the model. In the following sub-questions, we will experiment with each of these paradigms and measure the effect on the model generalization. We recommend disabling Early Stopping from previous question (simply removing it from config file) so that it does not interrupt your experiments with data augmentations and you maintain full control over number of epochs.\n",
    "\n",
    "a) Data augmentation is the process of creating more training data by applying certain transformations to the training set images. Usually, the underlying assumption is that the label of the image does not change under the applied transformations. This includes geometric transformations like translation, rotation, scaling, flipping, random cropping, and color transformations like greyscale, colorjitter. For every image in the training batch, a random transformation is sampled from the possible ones (e.g., a random number of pixels to translate the image by) and is applied to the image. While designing the data input pipeline, we must choose the hyper-parameters for these transformations (e.g., limits of translation or rotation) based on things we expect to see in the test-set/real world. Your task in this question is to implement the data augmentation for the CIFAR-10 classification task. Many of these transformations are implemented in the `torchvision.transforms` package. Familiarize yourself with the APIs of these transforms, and functions to compose multiple transforms or randomly sample them. Next, implement geometric and color space data augmentations for the CIFAR-10 dataset, by choosing the right functions and order of application. Tune the hyper-parameters of these data augmentations to improve the validation performance. You will need to train the model a bit longer (20-30 epochs) with data augmentation, as the training data is effectively larger now. Discuss which augmentations work well for you in the report. (6 points)\n",
    "\n",
    "b) Dropout is a popular scheme to regularize the model to improve generalization. The dropout layer works by setting the input activations randomly to zero at the output. You can implement Dropout by adding the `torch.nn.Dropout` layer between the conv blocks in your model. The layer has a single hyper-parameter $p$, which is the probability of dropping the input activations. High values of $p$ regularize the model heavily and decrease model capacity, but with low values, the model might overfit. Find the right hyper-parameter for $p$ by training the model for different values of $p$ and comparing training validation and validation accuracies. You can use the same parameter $p$ for all layers. You can also disable the data augmentation from the previous step while running this experiment, to clearly see the benefit of dropout. Show the plot of training and validation accuracies for different values of dropout (0.1 - 0.9) in the report. (4 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wirte your report for Q3 in this cell. Feel free to add extra code cells\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AOgg_0Scc4uL"
   },
   "source": [
    "### Question 4: Use pretrained networks (10 points)\n",
    "\n",
    "It has become standard practice in computer vision tasks related to images to use a convolutional network pre-trained as the backbone feature extraction network and train new layers on top for the target task. In this question, we will implement such a model. We will use the `VGG_11_bn` network from the `torchvision.models` library as our backbone network. This model has been trained on ImageNet, achieving a top-5 error rate of 10.19%. It consists of 8 convolutional layers followed by adaptive average pooling and fully-connected layers to perform the classification. We will get rid of the average pooling and fully-connected layers from the `VGG_11_bn` model and attach our own fully connected layers to perform the CIFAR-10 classification.\n",
    "\n",
    "a) Instantiate a pretrained version of the `VGG_11_bn` model with ImageNet pre-trained weights. Add two fully connected layers on top, with Batch Norm and ReLU layers in between them, to build the CIFAR-10 10-class classifier. Note that you will need to set the correct mean and variance in the data-loader, to match the mean and variance the data was normalized with when the `VGG_11_bn` was trained. Train only the newly added layers while disabling gradients for the rest of the network. Each parameter in PyTorch has a required grad flag, which can be turned off to disable gradient computation for it. Get familiar with this gradient control mechanism in PyTorch and train the above model. As a reference point, you will see validation accuracies in the range (61-65%) if implemented correctly. (6 points)\n",
    "\n",
    "b) We can see that while the ImageNet features are useful, just learning the new layers does not yield better performance than training our own network from scratch. This is due to the domain-shift between the ImageNet dataset (224x224 resolution images) and the CIFAR-10 dataset (32x32 images). To improve the performance we can fine-tune the whole network on the CIFAR-10 dataset, starting from the ImageNet initialization (set `\"fine_tune\"` to `true` in `vgg_cifar10.json`). To do this, enable gradient computation to the rest of the network, and update all the model parameters. Additionally train a baseline model where the same entire network is trained from scratch, without loading the ImageNet weights (set `\"pretrained\"` to `false` in `vgg_cifar10.json`). Compare the two models' training curves, validation, and testing performance in the report. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.vgg_trainer import VGGTrainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By default WandB is enabled in config file for vgg_cifar10.json. You can set it to false if you don't want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser.wo_args(config='cfgs/exercise-3/vgg_cifar10.json', root_dir=PROJECT_ROOT)\n",
    "\n",
    "wandb_enabled = config['wandb']\n",
    "if wandb_enabled:\n",
    "    import wandb\n",
    "\n",
    "datamodule = config.init_obj('data_module', module_data,\n",
    "                             root_dir=PROJECT_ROOT #Just add the root_dir to the rest of the arguments defined in config file\n",
    "                            )\n",
    "\n",
    "# Based on the heldout_split in the config file, \n",
    "# the datamodule will break the dataset into two splits\n",
    "train_data_loader = datamodule.get_loader()\n",
    "valid_data_loader = datamodule.get_heldout_loader()\n",
    "\n",
    "if wandb_enabled:\n",
    "    # change entity to your wandb username/group name. Also feel free to rename project and run names.\n",
    "    run = wandb.init(project=\"hlcv-exercise-3\", name=\"vgg_cifar10\", config=config, entity=\"malouda-triangle\", dir=PROJECT_ROOT)\n",
    "    run.name = run.name + f'-{run.id}'\n",
    "    assert run is wandb.run\n",
    "\n",
    "trainer_vgg = VGGTrainer(config=config, train_loader=train_data_loader, eval_loader=valid_data_loader)\n",
    "\n",
    "# Test loader is the same as train loader\n",
    "# except that training=False, shuffle=False, and no splitting is done \n",
    "# So we use the exact config from training and just modify these arguments\n",
    "test_loader_args = deepcopy(config['data_module']['args']) #copy the args\n",
    "test_loader_args['training']=False\n",
    "test_loader_args['shuffle']=False\n",
    "test_loader_args['heldout_split']=0.0\n",
    "\n",
    "# Now we initialize the test module with the modified config\n",
    "test_module = getattr(module_data, config['data_module']['type'])(root_dir=PROJECT_ROOT, **test_loader_args)\n",
    "# And get the loader from it\n",
    "test_loader = test_module.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_vgg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the experiment you want to visualize (format is `MMDD_TIME`)\n",
    "checkpoint_dir = '0523_215742' \n",
    "path = ospj(PROJECT_ROOT, f'saved/models/CIFAR10_VGG/{checkpoint_dir}/best_val_model.pth')\n",
    "\n",
    "trainer_vgg.load_model(path=path)\n",
    "\n",
    "result = trainer_vgg.evaluate(loader=test_loader)\n",
    "\n",
    "print(result)\n",
    "\n",
    "if wandb_enabled:\n",
    "    for metrics, values in result.items():\n",
    "        wandb.run.summary[f\"test_{metrics}\"] = values\n",
    "\n",
    "    # Change the title and message as you wish. Would only work if you have enabled push notifications for your email/slack in wandb account settings.\n",
    "    wandb.alert(title=\"Training Finished\", text=f'VGG Training has finished. Test results: {result}', level=wandb.AlertLevel.INFO)\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uvjz11dc4uN"
   },
   "source": [
    "#### Write your report for Q4 in this cell.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
