{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Colab, comment out if not using Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line if you're using Colab to something like '/content/drive/MyDrive/TeamX/'\n",
    "# where TeamX is just the clone of repository on your Google Drive\n",
    "# and you have mounted the drive at /content/drive  \n",
    "# See the Tutorial Slides for more detail.\n",
    "\n",
    "# Works on your local machine but not on Colab!\n",
    "PROJECT_ROOT = '/icbb/projects/igunduz/hlcv-project' \n",
    "\n",
    "# Fix this path and use this one on Colab\n",
    "# PROJECT_ROOT = '/content/drive/MyDrive/TeamX' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import join as ospj\n",
    "\n",
    "sys.path.append(ospj(PROJECT_ROOT, 'src'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with the data\n",
    "### 1. Load the data\n",
    "Using the IIT-AFF dataset, we will load the data and see what it looks like.\n",
    "### 2. Visualize the data\n",
    "We will visualize the data to see what it looks like.\n",
    "### 3. Preprocess the data\n",
    "Create a DataLoaders object for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "\n",
    "data_dir = ospj(PROJECT_ROOT, 'data', 'IIT_Affordances_2017', 'IIT_Affordances_2017')\n",
    "\n",
    "background = (200, 222, 250)\n",
    "c1 = (0,0,205)   \n",
    "c2 = (34,139,34) \n",
    "c3 = (192,192,128)   \n",
    "c4 = (165,42,42)    \n",
    "c5 = (128,64,128)   \n",
    "c6 = (204,102,0)  \n",
    "c7 = (184,134,11) \n",
    "c8 = (0,153,153)\n",
    "c9 = (0,134,141)\n",
    "c10 = (184,0,141) \n",
    "c11 = (184,134,0) \n",
    "c12 = (184,134,223)\n",
    "label_colours = np.array([background, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12])\n",
    "\n",
    "# Object\n",
    "col0 = (0, 0, 0)\n",
    "col1 = (0, 255, 255)\n",
    "col2 = (255, 0, 255)\n",
    "col3 = (0, 125, 255)\n",
    "col4 = (55, 125, 0)\n",
    "col5 = (255, 50, 75)\n",
    "col6 = (100, 100, 50)\n",
    "col7 = (25, 234, 54)\n",
    "col8 = (156, 65, 15)\n",
    "col9 = (215, 25, 155)\n",
    "col10 = (25, 25, 155)\n",
    "\n",
    "col_map = [col0, col1, col2, col3, col4, col5, col6, col7, col8, col9, col10]\n",
    "\n",
    "\n",
    "# img = Image.open(ospj(data_dir, 'rgb', '00_00000090.jpg'))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(math.sqrt(562500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Text File to Segmentation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affordance_path = ospj(data_dir, 'affordances_labels', 'ILSVRC2014_train_00044663.txt')\n",
    "\n",
    "with open(affordance_path) as f:\n",
    "    affordance_labels = f.readlines()\n",
    "    dim1 = len(affordance_labels)\n",
    "    dim2 = len(affordance_labels[0].split())\n",
    "    \n",
    "count = 0\n",
    "\n",
    "# Creation of Affordance pixel grid\n",
    "# Checking image size to continue with creating Segmentation Map\n",
    "affordance_pixels = []\n",
    "for label in affordance_labels:\n",
    "    row_pixels = []\n",
    "    digits = label.split()\n",
    "    count = 0\n",
    "    for digit in digits:\n",
    "        if digit.isdigit():\n",
    "            row_pixels.append(digit)\n",
    "            count += 1\n",
    "    assert count==dim2, f\"Actual count: {count}\"\n",
    "    affordance_pixels.append(row_pixels)\n",
    "        \n",
    "print(count)\n",
    "\n",
    "dimensions = [dim1, dim2]\n",
    "print(dimensions)\n",
    "\n",
    "affordance_pixels = np.array(affordance_pixels, dtype=np.uint8)\n",
    "print(len(affordance_pixels))\n",
    "\n",
    "seg_map = label_colours[affordance_pixels]\n",
    "print(seg_map.shape)\n",
    "\n",
    "seg_map = np.array(seg_map, dtype=np.uint8)\n",
    "print(seg_map.shape)\n",
    "\n",
    "# bgr_image = cv2.cvtColor(seg_map, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "plt.imshow(seg_map)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_141 = cv2.imread(ospj(data_dir, 'rgb', 'ILSVRC2014_train_00044663.jpg'))\n",
    "print(f\"Image shape: {img_141.shape}\")\n",
    "print(f\"Mask shape: {seg_map.shape}\")\n",
    "plt.imshow(img_141)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor, SegformerForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "seg_b0_feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/mit-b0\")\n",
    "seg_b0_encoder_model = SegformerForImageClassification.from_pretrained(\"nvidia/mit-b0\")\n",
    "\n",
    "# inputs = seg_b0_feature_extractor(images=img_141, segmentation_maps=seg_map, return_tensors=\"pt\")\n",
    "\n",
    "# print(inputs.keys())\n",
    "\n",
    "# outputs = seg_b0_encoder_model(**inputs)\n",
    "# logits = outputs.logits\n",
    "# # model predicts one of the 1000 ImageNet classes\n",
    "# predicted_class_idx = logits.argmax(-1).item()\n",
    "# print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seg_b0_encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "seg_b0_processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "seg_b0_model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "print(seg_b0_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(ospj(data_dir, 'rgb', 'ILSVRC2014_train_00044663.jpg'))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor_1(images=img_141, bgr_image, return_tensors=\"pt\")\n",
    "outputs = model_1(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.mean(logits.squeeze().detach().numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_dir = ospj(PROJECT_ROOT, 'data', 'VOCdevkit2012', 'VOC2012', 'JPEGImages')\n",
    "img = Image.open(ospj(data_dir, '1347.jpg'))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
    "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor(images=img, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# model predicts bounding boxes and corresponding COCO classes\n",
    "logits = outputs.logits\n",
    "bboxes = outputs.pred_boxes\n",
    "\n",
    "\n",
    "# print results\n",
    "target_sizes = torch.tensor([img.size[::-1]])\n",
    "results = image_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders.AffordanceDataset import AffordanceDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "\n",
    "\n",
    "\n",
    "transform = Compose([Resize((600,600)),\n",
    "                     ToTensor()])\n",
    "\n",
    "# transform = None\n",
    "\n",
    "train_dataset = AffordanceDataset(root_dir=data_dir,\n",
    "                                split_file=\"train_and_val.txt\",\n",
    "                                feature_extractor=seg_b0_feature_extractor,\n",
    "                                transform=transform)\n",
    "\n",
    "validation_dataset = AffordanceDataset(root_dir=data_dir, \n",
    "                                  split_file=\"val.txt\",\n",
    "                                  feature_extractor=seg_b0_feature_extractor, \n",
    "                                  transform=transform)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in validation_loader:\n",
    "    input = batch['image']\n",
    "    target = batch['affordances_labels']\n",
    "    print(input.shape)\n",
    "    print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
